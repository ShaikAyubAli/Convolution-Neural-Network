{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d626397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d002b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c9a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c891d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "3.0.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__) \n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c269d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataGenerator(rescale = 1/255,shear_range=0.2,zoom_range =0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866bf52",
   "metadata": {},
   "source": [
    "- **rescale = 1/255**: This argument scales the pixel values of the images by a factor of 1/255, so that they are in the range of [0, 1] instead of [0, 255]. This is a common preprocessing step for neural networks that work with images.\n",
    "- **shear_range = 0.2**: This argument applies a random shear transformation to the images, which means that the images are skewed along an axis by a certain angle. **The angle is chosen randomly from the range of [-0.2, 0.2] radians**. Shear transformation can help the model learn to recognize objects that are not perfectly aligned or oriented.\n",
    "- **zoom_range = 0.2**: This argument applies a random zoom transformation to the images, which means that the images are scaled up or down by a certain factor. **The factor is chosen randomly from the range of [1 - 0.2, 1 + 0.2]**. Zoom transformation can help the model learn to recognize objects that are at different distances or sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a12cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_data.flow_from_directory(\"dataset/training_set\",target_size = (64,64),class_mode = \"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ae382",
   "metadata": {},
   "source": [
    "- **target_size = (64,64)** # customize (64,64) size accordingly with the least size of your images data\n",
    "- **class_mode = \"binary\"** # the labels will be 1D binary arrays, such as [0] for dog and [1] for cat. \n",
    "- **class_mode = \"categorical\" or \"sparse\"** # If there are more than two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b948ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd39f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "test_set = test_data.flow_from_directory('dataset/test_set',target_size = (64,64),class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a41db9",
   "metadata": {},
   "source": [
    "## Modelling - Convolution Neural Network \n",
    "#### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d300cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f689ef7",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0e7e6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junai\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "classifier.add(Conv2D(input_shape=[64,64,3],filters=32,kernel_size=3,activation=\"relu\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b20de",
   "metadata": {},
   "source": [
    "- **input_shape=[64,64,3]** # specifies the shape of the input images. In this case, the images have a height of 64 pixels, a width of 64 pixels, and 3 color channels (RGB).\n",
    "- **filters=32** --> **Max** # specifies the number of filters that the convolution layer will learn. Each filter is a small matrix that slides over the input image and produces a scalar value for each position. The output of applying all the filters to the input image is a feature map that has the same height and width as the input, but a depth equal to the number of filters.\n",
    "- **kernel_size=3** # specifies the size of the filters. In this case, the filters are 3x3 matrices. The kernel size determines the receptive field of the filters, which is the area of the input image that influences the output of each filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778726a",
   "metadata": {},
   "source": [
    "### Step 2 - Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e086853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "classifier.add(MaxPooling2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7bcf3",
   "metadata": {},
   "source": [
    "- **pool_size = 2**: This argument specifies the size of the pooling regions, which are rectangular windows that slide over the input tensor. The size can be either an integer or a tuple of 2 integers, representing the height and width of the pooling regions. In your case, the pooling regions have a size of 2x2 pixels.\n",
    "- **strides = 2**: This argument specifies the strides of the pooling regions, which are the distances between two consecutive pooling regions along each dimension. The strides can be either None, an integer, or a tuple of 2 integers. If None, the strides will default to the pool size. In your case, the strides are 2x2 pixels, meaning that the pooling regions are non-overlapping.\n",
    "- The size and resolution of the input images: Larger images may require larger pooling regions and strides to reduce the dimensionality and avoid overfitting. Smaller images may require smaller pooling regions and strides to preserve the information and avoid underfitting.\n",
    "- The complexity and diversity of the features: More complex and diverse features may require smaller pooling regions and strides to capture the details and variations. Less complex and diverse features may require larger pooling regions and strides to reduce the noise and redundancy.\n",
    "- The architecture and depth of the model: Deeper models may require smaller pooling regions and strides to avoid losing too much information and resolution in the lower layers. Shallower models may require larger pooling regions and strides to achieve sufficient downsampling and abstraction in the higher layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c3d53",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6887e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bc2f0",
   "metadata": {},
   "source": [
    "### Step 4 - Full Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc5db238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# Hidden layer with 128 neurons\n",
    "classifier.add(Dense(units=128,activation=\"relu\"))\n",
    "\n",
    "# Output Layer with 1 neuron\n",
    "classifier.add(Dense(units=1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c690123",
   "metadata": {},
   "source": [
    "#### Training the CNN Model with Train data & Testing the Model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e04a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ac5eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junai\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 152ms/step - accuracy: 0.5107 - loss: 0.8281 - val_accuracy: 0.5526 - val_loss: 0.6868\n",
      "Epoch 2/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 125ms/step - accuracy: 0.6493 - loss: 0.6287 - val_accuracy: 0.6969 - val_loss: 0.5899\n",
      "Epoch 3/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 138ms/step - accuracy: 0.6952 - loss: 0.5749 - val_accuracy: 0.7262 - val_loss: 0.5491\n",
      "Epoch 4/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 148ms/step - accuracy: 0.7164 - loss: 0.5533 - val_accuracy: 0.7341 - val_loss: 0.5352\n",
      "Epoch 5/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - accuracy: 0.7449 - loss: 0.5182 - val_accuracy: 0.7475 - val_loss: 0.5269\n",
      "Epoch 6/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 139ms/step - accuracy: 0.7535 - loss: 0.5025 - val_accuracy: 0.7465 - val_loss: 0.5239\n",
      "Epoch 7/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 145ms/step - accuracy: 0.7681 - loss: 0.4779 - val_accuracy: 0.7560 - val_loss: 0.5363\n",
      "Epoch 8/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 155ms/step - accuracy: 0.7819 - loss: 0.4600 - val_accuracy: 0.7604 - val_loss: 0.5234\n",
      "Epoch 9/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 153ms/step - accuracy: 0.7857 - loss: 0.4525 - val_accuracy: 0.7525 - val_loss: 0.5426\n",
      "Epoch 10/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 116ms/step - accuracy: 0.7869 - loss: 0.4413 - val_accuracy: 0.7560 - val_loss: 0.5494\n",
      "Epoch 11/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 120ms/step - accuracy: 0.7901 - loss: 0.4353 - val_accuracy: 0.7579 - val_loss: 0.5286\n",
      "Epoch 12/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 117ms/step - accuracy: 0.8115 - loss: 0.4110 - val_accuracy: 0.7639 - val_loss: 0.5305\n",
      "Epoch 13/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 116ms/step - accuracy: 0.8122 - loss: 0.4138 - val_accuracy: 0.7421 - val_loss: 0.5882\n",
      "Epoch 14/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - accuracy: 0.8136 - loss: 0.4050 - val_accuracy: 0.7515 - val_loss: 0.5697\n",
      "Epoch 15/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - accuracy: 0.8229 - loss: 0.3801 - val_accuracy: 0.7505 - val_loss: 0.5833\n",
      "Epoch 16/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 116ms/step - accuracy: 0.8278 - loss: 0.3805 - val_accuracy: 0.7644 - val_loss: 0.5457\n",
      "Epoch 17/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 124ms/step - accuracy: 0.8322 - loss: 0.3695 - val_accuracy: 0.7569 - val_loss: 0.5614\n",
      "Epoch 18/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 126ms/step - accuracy: 0.8311 - loss: 0.3708 - val_accuracy: 0.7629 - val_loss: 0.5811\n",
      "Epoch 19/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 121ms/step - accuracy: 0.8553 - loss: 0.3403 - val_accuracy: 0.7713 - val_loss: 0.5627\n",
      "Epoch 20/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - accuracy: 0.8521 - loss: 0.3311 - val_accuracy: 0.7664 - val_loss: 0.5741\n",
      "Epoch 21/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.8609 - loss: 0.3257 - val_accuracy: 0.7242 - val_loss: 0.7036\n",
      "Epoch 22/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - accuracy: 0.8592 - loss: 0.3154 - val_accuracy: 0.7530 - val_loss: 0.6167\n",
      "Epoch 23/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 117ms/step - accuracy: 0.8656 - loss: 0.3056 - val_accuracy: 0.7688 - val_loss: 0.6269\n",
      "Epoch 24/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - accuracy: 0.8701 - loss: 0.2990 - val_accuracy: 0.7614 - val_loss: 0.6387\n",
      "Epoch 25/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 121ms/step - accuracy: 0.8806 - loss: 0.2873 - val_accuracy: 0.7693 - val_loss: 0.6181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b6ad37c290>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=training_set,validation_data = test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11806745",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "#### Making a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4e77f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a72c0f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "test_image = Image.open(\"dataset/single_prediction/cat_or_dog_1.jpg\")\n",
    "\n",
    "# Data Preprocessing\n",
    "test_image = test_image.resize((64,64))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "\n",
    "# Prediction\n",
    "result = classifier.predict(test_image)\n",
    "\n",
    "# Evaluation\n",
    "if result[0][0] ==1:\n",
    "    print(\"Dog\")\n",
    "else:\n",
    "    print(\"Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016d9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
